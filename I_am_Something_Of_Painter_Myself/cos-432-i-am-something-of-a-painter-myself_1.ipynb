{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I'm Something of a Painter Myself\n## Akriti Prajapati, Gaurav Aryal, Gaurav Aryal\n\n### The project we worked off of: https://www.kaggle.com/xyzymir/cycle-gan-pytorch\n\n#### Helper Projects: \n    https://www.kaggle.com/atrisaxena/pytorch-cyclegan\n    https://www.kaggle.com/nachiket273/cyclegan-pytorch\n    https://www.kaggle.com/adrianda/cyclegan-pytorch-style-transfer\n\nEvery artist has their own style that they demonstrate in their art pieces. This project is to bring Claude Monte’s style into photos or create the style from scratch. Generative adversarial networks (GANs) is a class of machine learning frameworks which made imitation of those arts possible. In this project, the task is to build GAN that generates 7,000 images in the style of Monet."},{"metadata":{},"cell_type":"markdown","source":"### Brief intro on what we changed:\nWe took the above mentioned project as a base project and started working forward with it. Following are what we have changed or added along the way:\n\n1. Set-up\n-> We added necessary imports as we made changes to the project\n\n2. DataSet\n-> Instead of using cv2.imread method from OpenCV library, we used \"Image\" method from PIL as it was easier to handle images through that method.\n\n3. Data Augmentation\n-> There are only 300 monet images while there are more than 7000 photos, so we decided to used data augmentation and add in some tweaked images into the monet dataset to increase its count. \n\n4. Reverse Normalize Method\n-> In order to undo the what the transforms.normalize does to our images.\n\n5. Test Images\n-> We plotted original photo and monet images to have a peak at what our datasets look like.\n\n6. CycleGAN Models\n-> We re-wrote all the helping functinos and discriminator, generator methods. \n\n7. Loss Functions \n-> Along with adversarial loss function and cycle consistency loss function, we also added in identity loss function to further ensure that the output from a mapping visually match the image from the domain they map to.\n\n8. Training Method\n-> The training method was quite well done, we did not change much in there but certainly did calculated all those loss functions.\n\n9. Image and Graph Plot\n-> After finishing the training, we plotted some images to compare and also a graph to showcase our training loss values.\n\n10. Submition\n-> We made some changes to accomodate our previous changes in PictureDataset."},{"metadata":{},"cell_type":"markdown","source":"## Set-up\nAll the packages are imported here."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom torch import tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport pandas as pd\nimport random\n\n# for loading images\nfrom pathlib import Path\n# import cv2\nfrom torch.utils.data import Dataset, DataLoader\nimport PIL\nfrom PIL import Image\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n\n\n# progress bar\nfrom fastprogress import master_bar, progress_bar\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path('../input/gan-getting-started')\nPath.BASE_PATH = path\nPath.ls = lambda x: [o.name for o in x.iterdir()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Datasets\nPictureDataset is returning a dataset for given path. Here, we have added some data augmentation. We have added some transformations to augment our data. As we already know, there are only 300 monet images which are very less with respect to the dataset of other photos. Inorder to increase the monet dataset, we concatenated augmented images with our original monet images. This method will augment our data whenever augment=True.\n\nHere, we have used the inbuild functionalitites of transtorchvision.transforms such as Resize, CenterCrop, RandomHorizontalFlip and CenterCrop to transform our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"class PictureDataset(Dataset):\n    def __init__(self, df, folder_path, img_size=256, augment=True):\n        super().__init__()\n        self.df = df\n        self.folder_path = folder_path\n        \n        # Normalization helps get data within a range and reduces the skewness \n        # which helps learn faster and better\n        # Normalize does the following for each channel: image = (image - mean) / std \n        # The parameters mean, std are passed as 0.5 and 0.5 respectively. \n        # This will normalize the image in the range [-1,1]\n        if augment:\n            self.transform = transforms.Compose([\n                transforms.Resize(img_size),\n                transforms.CenterCrop(256),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.Resize(img_size),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img = Image.open(str(self.folder_path/self.df[0][idx]))\n        img = self.transform(img)\n        \n        return img\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reverse Normalization\nThis unnormalize_img method undo what normalization does.\nThe zip() function takes in iterables as arguments and returns an iterator. This iterator generates a series of tuples containing elements from each iterable. We iterate through the given mean and standard deviation values to reverse what we did in normalization."},{"metadata":{"trusted":true},"cell_type":"code","source":"def unnormalize_img(img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n    for t, m, s in zip(img, mean, std):\n        t.mul_(s).add_(m)\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dataframe\ndf_photo = pd.DataFrame((path/'photo_jpg').ls())\ndf_monet = pd.DataFrame((path/'monet_jpg').ls())\n\n# photo dataset\nphotoDS = PictureDataset(df_photo, path/'photo_jpg', img_size=256, augment=False)\n\n# original monet dataset\noriginal_monet = PictureDataset(df_monet, path/'monet_jpg', img_size=256, augment=False)\n#transformed monet dataset\naugmented_monet = PictureDataset(df_monet, path/'monet_jpg', img_size=300, augment=True)\n# monetDS is the dataset which contains both the augmented and original monet images\nmonetDS = torch.utils.data.ConcatDataset([augmented_monet, original_monet])\n\nbatch_size = 100\n\n# create loaders\nphoto_loader = DataLoader(photoDS, batch_size, shuffle=True, num_workers=0, drop_last=True)\nmonet_loader = DataLoader(monetDS, batch_size, shuffle=True, num_workers=0, drop_last=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Images\nWe have plotted some photos and monet images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a test dataloader and plot the monet and photo images\ntest_batch_size = 3\ntest_photo_dataloader = DataLoader(photoDS, test_batch_size, shuffle=True, num_workers=0, drop_last=True)\ntest_monet_dataloader = DataLoader(monetDS, test_batch_size, shuffle=True, num_workers=0, drop_last=True)\n\n# plot the photo images\nimg_iter = iter(test_photo_dataloader)\nimg = img_iter.next()\ngrid_normalized = make_grid(img, nrow=4)\ngrid_original = unnormalize_img(grid_normalized)\nfig = plt.figure(figsize=(8, 8))\nplt.title('photo')\nplt.imshow(grid_original.permute(1, 2, 0).detach().numpy())\nplt.show()\n\n# plot the monet images\nimg_iter = iter(test_monet_dataloader)\nimg = img_iter.next()\ngrid_normalized = make_grid(img, nrow=4)\ngrid_original = unnormalize_img(grid_normalized)\nfig = plt.figure(figsize=(8, 8))\nplt.title('monet')\nplt.imshow(grid_original.permute(1, 2, 0).detach().numpy())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building blocks\nThere are three helping blocks.\nThe ConvBlock returns sequence of convolutional layer followed by batch normalization if batch_norm=True.\nWe previously chose to place instance normalization along with batch normalization. Batch normalization computes one mean and std dev (thus making the distribution of the whole layer Gaussian), instance normalization computes T (the number of input tensors) of them, making each individual image distribution look Gaussian, but not jointly. Before we used only 1 batch size so it was more benefitial to use instance normalization but now we are using 100 batch sizes, so it made more sense to use batch normalization and it generated better monet picture however the generator loss keeps increasing."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ConvBlock(in_channels , out_channels , kernel_size=4 , strides=2, padding=1, batch_norm=True):\n    layers = []      \n    conv_layer = nn.Conv2d(in_channels=in_channels , out_channels=out_channels , \n                          kernel_size=kernel_size , stride=strides , padding=padding ,  bias=False)\n    layers.append(conv_layer)\n    if(batch_norm):\n        layers.append(nn.BatchNorm2d(out_channels))\n        \n    return nn.Sequential(*layers)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For DeconvBlock, we initially had just a transpose convolutional layer with optional batch normalization but it started to leave checkboard marks on the generated images. After one of our calssmates presentation where he suspected the reason could be because of deconvolution operation. We did a little more research and found it to be true so then we tried and implemented the nn.Upsample method from the referred project which seem to have solved the checkboard issue.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DeconvBlock(nn.Sequential):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, batch_norm=True):\n        super().__init__(\n            nn.Upsample(scale_factor=2),\n            ConvBlock(in_channels, out_channels, kernel_size, strides=1, padding=padding, batch_norm=batch_norm)\n        )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ResidualBlock consists of two convolutional layers followed by batch normalization and relu activation function where a residue of input is added to the output. This is done to ensure properties of input of previous layers are available for later layers as well, so that their output do not deviate much from original input. Otherwise the characteristics of original images will not be retained in the output and results will be very abrupt."},{"metadata":{"trusted":true},"cell_type":"code","source":" class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_features, in_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(in_features),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return x + self.layers(x)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generator\nThe generator consists of three convolutional blocks, followed by a few resnet blocks, then three deconvoltional blocks.\n\nWe used leaky relu as the activation function for encoder and decoder and just relu function in residual block. The leaky relu is used to avoid the \"dead relu\" problem which happens when your ReLU always have values under 0. The residual block connects the output of one layer with the input of an earlier layer, so even with relu, we might lose some characteristics but we won't face dead relu."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CGanGenerator(nn.Module):\n    def __init__(self, conv_dim=8, res_layers=6):\n        super(CGanGenerator, self).__init__()\n\n        #Encoder layers\n        self.conv1 = ConvBlock(in_channels=3, out_channels=conv_dim, kernel_size=5, padding=2)\n        self.conv2 = ConvBlock(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=3, batch_norm=True)\n        self.conv3 = ConvBlock(in_channels=conv_dim*2, out_channels=conv_dim*4, kernel_size=3, batch_norm=True)\n        \n        #Residual blocks\n        residual_layers = []\n        for layer in range(res_layers):\n            residual_layers.append(ResidualBlock(conv_dim*4))\n        self.res_blocks = nn.Sequential(*residual_layers)\n        \n        #Decoder layers\n        self.deconv4 = DeconvBlock(in_channels=conv_dim*4, out_channels=conv_dim*2, kernel_size=3, batch_norm=True)\n        self.deconv5 = DeconvBlock(in_channels=conv_dim*2, out_channels=conv_dim, kernel_size=3, batch_norm=True)\n        self.deconv6 = DeconvBlock(in_channels=conv_dim, out_channels=3, kernel_size=3, batch_norm=True)\n        \n    def forward(self, x):\n        #Encoder\n        out = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n        out = F.leaky_relu(self.conv2(out), negative_slope=0.2)\n        out = F.leaky_relu(self.conv3(out), negative_slope=0.2)\n        \n        #Residual blocks\n        out = self.res_blocks(out)\n        \n        #Decoder\n        out = F.leaky_relu(self.deconv4(out), negative_slope=0.2)\n        out = F.leaky_relu(self.deconv5(out), negative_slope=0.2)\n        out = torch.tanh(self.deconv6(out))\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discriminator\nEach generator has a corresponding discriminator model.\nOur discriminator consists of bucnh of convlolutional layers. The convolution network help to extract features from the images which may be helpful in classifying the objects in that image. It starts by extracting low dimensional features (like edges) from the image, and then some high dimensional features like the shapes. We implemented a simplified CycleGAN discriminator, which is a network of 6 convolution layers with optional batch normalization including:\n5 layers to extract features from the image, and\n1 layer to produce the output (whether the image is fake or not)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CGanDiscriminator(nn.Module):\n    def __init__(self, conv_dim=8):\n        super(CGanDiscriminator, self).__init__()\n        \n        #convolutional layers, increasing in depth\n        self.conv1 = ConvBlock(in_channels=3, out_channels=conv_dim, kernel_size=5)\n        self.conv2 = ConvBlock(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=3, batch_norm=True)\n        self.conv3 = ConvBlock(in_channels=conv_dim*2, out_channels=conv_dim*4, kernel_size=3, batch_norm=True)\n        self.conv4 = ConvBlock(in_channels=conv_dim*4, out_channels=conv_dim*8, kernel_size=3, batch_norm=True)\n        self.conv5 = ConvBlock(in_channels=conv_dim*8, out_channels=conv_dim*8, kernel_size=3, batch_norm=True)\n        self.conv6 = ConvBlock(conv_dim*8, out_channels=1, kernel_size=3, strides=2)\n    \n    def forward(self, x):\n        #leaky relu applied to all conv layers but last\n        out = F.leaky_relu(self.conv1(x), negative_slope=0.2)\n        out = F.leaky_relu(self.conv2(out), negative_slope=0.2)\n        out = F.leaky_relu(self.conv3(out), negative_slope=0.2)\n        out = F.leaky_relu(self.conv4(out), negative_slope=0.2)\n        out = F.leaky_relu(self.conv5(out), negative_slope=0.2)\n        out = torch.sigmoid(self.conv6(out))\n       \n        return out ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build model and optimizer\nWe have build two generators which converts monet into photo and photo into monet respectively. Then we have two generators.\n\nOptimizers help to adjust the properties of a network, such as weights and learning rate to minimize the losses. Adadelta is an Adagrad extension reduces the aggressive decreasing learning rate. Adadelta limits the window of cumulative past gradients to a certain fixed size w, instead of accumulating all previous squared gradients.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"monet_to_photo = CGanGenerator(res_layers=2).to(device)\nphoto_to_monet = CGanGenerator(res_layers=2).to(device)\n\nmonet_dis = CGanDiscriminator().to(device) # checks photo->monet\nphoto_dis = CGanDiscriminator().to(device) # checks monet->photo\n\nopt_m2p = optim.Adadelta(monet_to_photo.parameters())\nopt_p2m = optim.Adadelta(photo_to_monet.parameters())\n\nopt_mdis = optim.Adadelta(monet_dis.parameters())\nopt_pdis = optim.Adadelta(photo_dis.parameters())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loss Function\nThe project had defined adversarial loss function and cycle consistenct loss function within the training loop but we wanted to separate them into a new function and add in another loss function: identity loss function.\n\n#### Adversarial Loss Function\nBoth generators are attempting to “fool” their corresponding discriminator into being less able to distinguish their generated images from the real versions. We used the least squares loss to capture this.\n\n#### Cycle Consistency Loss Function\nHowever, the adversarial loss alone is not sufficient to produce good images, as it leaves the model under-constrained. It enforces that the generated output be of the appropriate domain, but does not enforce that the input and output are recognizably the same. For example, a generator that output an image y that was an excellent example of that domain, but looked nothing like x, would do well by the standard of the adversarial loss, despite not giving us what we really want.\n\nThe cycle consistency loss addresses this issue. It relies on the expectation that if you convert an image to the other domain and back again, by successively feeding it through both generators, you should get back something similar to what you put in. It enforces that F(G(x)) ≈ x and G(F(y)) ≈ y.\n\n#### Identity Loss Function\nThe identity loss function further ensures that the outputs from a mapping visually match the image from the domain they map to.\n\nThis loss can regularize the generator to be near an identity mapping when real samples of the target domain are provided. If something already looks like from the target domain, you should not map it into a different image. It  is used to preserve the color and prevent reverse color in the result. We calculate identity loss by doing absolute difference of generated image from real image times some identity weight.\n\nForward identity loss X->Y. Backward identity loss from Y->X.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adversarial Loss\ndef real_mse_loss(D_out, adverserial_weight=1):\n    return torch.mean((D_out-1)**2) * adverserial_weight\n\n# Adversarial Loss\ndef fake_mse_loss(D_out, adverserial_weight=1):\n    return torch.mean(D_out**2) * adverserial_weight\n\n# Cycle Consistency Loss\ndef cycle_consistency_loss(real_img, reconstructed_img, lambda_weight=1):\n    reconstr_loss = torch.mean(torch.abs(real_img - reconstructed_img))\n    return lambda_weight * reconstr_loss \n\n# Identity Loss\ndef identity_loss(real_img, generated_img, identity_weight=0.5):\n    ident_loss = torch.mean(torch.abs(real_img - generated_img))\n    return identity_weight * ident_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Loop\nThe training method ModelTraining was well formed in the referred project, however we added some loss functions and printed the progress."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ModelTraining(epochs, discriminator_epochs = 5):\n    mb = master_bar(range(epochs))\n\n    losses = [] #losses over all iteration\n    \n    # additional weighting parameters\n    adverserial_weight = 0.5\n    lambda_weight = 10\n    identity_weight = 5\n    \n    #Average loss over batches per epoch runs\n    d_total_loss_avg = 0.0\n    g_total_loss_avg = 0.0\n    \n    # minimum number of images in the iteration\n    monet_iter = iter(monet_loader)\n    img_per_iteration = len(monet_iter)\n    \n    for epoch in mb:\n        # do a few discriminator epochs\n        \n        # declare losses that will be used in reach batch iteration\n        # monet discriminator losses\n        realMonetLoss = 0.0\n        fakeMonetLoss = 0.0\n        # photo discriminator losses\n        realPhotoLoss = 0.0\n        fakePhotoLoss = 0.0\n        # photo to monet generator losses\n        gen_photo_to_monet_loss = 0.0\n        cyc_loss_fake_photo = 0.0\n        identity_fake_photo_loss = 0.0\n        # monet to photo generator losses\n        gen_monet_to_photo_loss = 0.0\n        cyc_loss_fake_monet = 0.0\n        identity_fake_monet_loss = 0.0\n        \n        monet_dis.train() \n        photo_dis.train() \n\n        monet_to_photo.eval()\n        photo_to_monet.eval()\n        \n        do_epochs = discriminator_epochs if epoch % 100 != 0 else discriminator_epochs * 10\n\n        for _ in progress_bar(range(do_epochs), parent=mb):\n\n            # train monet discriminator: monet_dis (#photo->monet)\n            monet_iter = iter(monet_loader)\n            photo_iter = iter(photo_loader)\n\n            for monet_batch in monet_iter:\n                monet_batch = monet_batch.to(device)\n\n                opt_mdis.zero_grad()\n                \n                real_monet = monet_dis(monet_batch)\n                fake_monet = photo_to_monet(next(photo_iter).to(device))\n                \n                # compute the monet discriminator losses on real images\n                realMonetLoss = real_mse_loss(real_monet, adverserial_weight)\n                # compute the monet discriminator losses on fake images\n                fakeMonetLoss = fake_mse_loss(fake_monet, adverserial_weight)\n                total_monet_dis_loss = realMonetLoss + fakeMonetLoss\n                \n                # do back propagation\n                total_monet_dis_loss.backward()\n                opt_mdis.step()\n\n            # train photo discriminator (#monet->photo)\n            monet_iter = iter(monet_loader)\n            photo_iter = iter(photo_loader)\n\n            for monet_batch in monet_iter:\n                monet_batch = monet_batch.to(device)\n                photo_batch = next(photo_iter).to(device)\n\n                opt_pdis.zero_grad()\n                \n                real_photo = photo_dis(photo_batch)\n                fake_photo = monet_to_photo(monet_batch)\n                \n                # compute the photo discriminator losses on real images\n                realPhotoLoss = real_mse_loss(real_photo, adverserial_weight)\n                # compute the photo discriminator losses on fake images\n                fakePhotoLoss = fake_mse_loss(fake_photo, adverserial_weight)\n                total_photo_dis_loss = realPhotoLoss + fakePhotoLoss\n                \n                # do back propagation\n                total_photo_dis_loss.backward()\n                opt_pdis.step()\n\n        monet_iter = iter(monet_loader)\n        photo_iter = iter(photo_loader)\n\n        monet_to_photo.train()\n        photo_to_monet.train()\n\n        monet_dis.eval()\n        photo_dis.eval()\n\n        # train photo_to_monet generator (photo -> monet and generated monet -> reconstructed photo)\n        for _ in progress_bar(range(len(monet_iter)), parent=mb):\n            photo_batch = next(photo_iter).to(device)\n\n            opt_p2m.zero_grad()\n            opt_m2p.zero_grad()\n\n            # convert the photo image to monet style\n            fake_monet = photo_to_monet(photo_batch)\n            dis_fake_monet = monet_dis(fake_monet)\n            # compute generator loss based on monet domain\n            gen_photo_to_monet_loss = real_mse_loss(dis_fake_monet, adverserial_weight)\n            fake_photo = monet_to_photo(fake_monet)\n            # compute cycle consistency loss\n            cyc_loss_fake_photo = cycle_consistency_loss(photo_batch, fake_photo, lambda_weight)\n            # compute identity loss\n            identity_fake_photo_loss = identity_loss(photo_batch, fake_monet, identity_weight)\n            \n            total_gen_photo_to_monet_loss = gen_photo_to_monet_loss + cyc_loss_fake_photo + identity_fake_photo_loss\n            \n            # do back propagation on the total generator loss\n            total_gen_photo_to_monet_loss.backward()\n\n            opt_p2m.step()\n            opt_m2p.step()\n\n        # train monet_to_photo generator (monet -> photot and generated photo -> reconstructed monet)\n        for monet_batch in progress_bar(monet_iter, parent=mb):\n            monet_batch = monet_batch.to(device)\n\n            opt_p2m.zero_grad()\n            opt_m2p.zero_grad()\n\n            # convert the monet image to photo style\n            fake_photo = monet_to_photo(monet_batch)\n            dis_fake_photo = photo_dis(fake_photo)\n            # compute generator loss based on photo domain\n            gen_monet_to_photo_loss = real_mse_loss(dis_fake_photo, adverserial_weight)\n            fake_monet = photo_to_monet(fake_photo)\n            # compute cycle consistency loss\n            cyc_loss_fake_monet = cycle_consistency_loss(monet_batch, fake_monet, lambda_weight)\n            # compute identity loss\n            identity_fake_monet_loss = identity_loss(monet_batch, fake_photo, identity_weight)\n            \n            total_gen_monet_to_photo_loss = gen_monet_to_photo_loss + cyc_loss_fake_monet + identity_fake_monet_loss\n\n            # do back propagation on the total generator losses\n            total_gen_monet_to_photo_loss.backward()\n\n            opt_p2m.step()\n            opt_m2p.step() \n            \n        # calculate average discriminator and generator losses\n        d_total_loss = realMonetLoss + fakeMonetLoss + realPhotoLoss + fakePhotoLoss\n        d_total_loss_avg = d_total_loss_avg + d_total_loss/img_per_iteration\n        g_total_loss = gen_photo_to_monet_loss + cyc_loss_fake_photo + identity_fake_photo_loss + gen_monet_to_photo_loss + cyc_loss_fake_monet + identity_fake_monet_loss\n        g_total_loss_avg = g_total_loss_avg + g_total_loss/img_per_iteration\n        \n        # append discriminator losses and generator losses to the losses\n        losses.append((d_total_loss_avg.item(), g_total_loss_avg.item()))\n    \n    return losses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = ModelTraining(700)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a test dataloader and plot the monet and photo images\ntest_output_batch_size = 1\noutput_photo_dataloader = DataLoader(photoDS, test_output_batch_size, shuffle=True, num_workers=0, drop_last=True)\n_, ax = plt.subplots(3, 2, figsize=(8, 8))\n\nfor i in range(3):\n    img_iter = iter(output_photo_dataloader)\n    img = img_iter.next()\n    grid_normalized_photo = make_grid(img, nrow=4)\n    grid_original_photo = unnormalize_img(grid_normalized_photo)\n    \n    generated_monet = photo_to_monet(img.to(device)).cpu().detach()\n    grid_normalized_monet = make_grid(generated_monet, nrow=4)\n    grid_original_monet = unnormalize_img(grid_normalized_monet)\n    \n    ax[i, 0].imshow(grid_original_photo.permute(1, 2, 0).detach())\n    ax[i, 1].imshow(grid_original_monet.permute(1, 2, 0))\n    ax[i, 0].set_title(\"Photo\")\n    ax[i, 1].set_title(\"Monet\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot loss functions over training\nfig, ax = plt.subplots(figsize=(12,8))\nlosses = np.array(losses)\nplt.plot(losses.T[0], label='Discriminators', alpha=0.5)\nplt.plot(losses.T[1], label='Generators', alpha=0.5)\nplt.title(\"Training Losses\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission\nTo submit our result, we ran our photos through out photo_to_monet generator, did some transformations and then saved them in the images folder.\nSince we chaged the method of opening a image in the PictureDataset, we had to chagen the submission file as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_batch_size = 100\nphoto_final_ds = PictureDataset(df_photo, path/'photo_jpg')\nphoto_final_loader = DataLoader(photo_final_ds, batch_size=final_batch_size, drop_last=True)\n\nphoto_to_monet.eval()\n\n\npath_output = Path('./')\n!mkdir ../images\nfor i, photo_batch in enumerate(iter(photo_final_loader)):\n    monet_batch = photo_to_monet(photo_batch.to(device))\n    monet_batch = monet_batch.detach().cpu()\n    for j in range(final_batch_size):\n        monet = monet_batch[j]\n        monet = unnormalize_img(monet).numpy().astype(np.uint8)\n        monet = np.transpose(monet, [1, 2, 0])\n        monet = Image.fromarray(monet)\n        monet.save('../images/' +  str(i*final_batch_size + j) + '.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}